<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" href="style.css">
      </head>

    <body>
        <!-- path.component.html -->
<div class="container">
    <a href="/" class="back-link">⬅️ Back</a>
    
    <h1>Model Evaluation Flow</h1>

    <!--create waterfall list-->  
    <ul class="waterfall-list">
        <li class="waterfall-item">
            <h2>Discovering Model Output</h2>
            <p>The model is queried using the initial dataset to identify the corresponding label for each sample.</p>
        </li>
        <li class="waterfall-item">
            <h2>Crafting Adversarial Data</h2>
            <p>Adversarial data is generated by applying perturbations to the original dataset. These methods are categorized based on the attacker's level of knowledge.</p>
            <ul>
                <li>White-box: Full access to both model and data.</li>
                <li>Grey-box: Partial access—either the model or the data is known.</li>
                <li>Black-box: No internal knowledge is required; only model access is needed.</li>
            </ul>
        </li>
        <li class="waterfall-item">
            <h2>Attacker Objectives</h2>
            <p>A standard attack targets one or more of the following:</p>
            <ul>
                <li>Integrity: Degrade the model's predictive performance.</li>
                <li>Availability: Make the model unusable.</li>
                <li>Privacy: Extract or infer sensitive information from the model.</li>
            </ul>
            <p>A common method of compromising integrity is through evasion attacks, which aim to mislead the model during inference. These attacks can be:</p>
            <ul>
                <li>Targeted: The adversary attempts to trick the model into making a specific prediction.</li>
                <li>Non-targeted (Untargeted): The goal is simply to trick the model into making any incorrect prediction.</li>
            </ul>
            <p>These are also known as exploratory attacks, in which attackers gather information from the model in order to design effective inputs. Some attacks may also attempt to infer whether a given sample was used during training.</p>
        </li>
        <li class="waterfall-item">
            <h2>Data Quality Concerns</h2>
            <p>Most adversarial methods generate perturbations without considering the structural constraints of tabular data, particularly the presence of categorical features. This can result in generated values that are unrealistic or invalid. To address this issue, AURORA uses a Distance Adjustment (DA) that modifies evaluation metrics based on how much the adversarial samples deviate from the original data and whether the generated values are valid within the feature space.</p>
            </li>
        <li class="waterfall-item">
            <h2>Adversarial Metrics</h2>
            <p>To evaluate model performance under adversarial conditions, AURORA employs a set of metrics that offer more detailed insights. Some of these metrics require:</p>
            <ul>
                <li>Ground-truth labels for each sample.</li>
                <li>Specification of whether the attack is targeted.</li>
                </ul>
        </li>
        <li>
            <h2>Model Robustness</h2>
            <p>AURORA computes a robustness score ranging from 1 to 5 based on the chosen metrics:</p>
            <ul>
                <li>1: Not robust.</li>
                <li>2: Weakly robust.</li>
                <li>3: Moderately robust.</li>
                <li>4: Robust.</li>
                <li>5: Very robust.</li>
            </ul>
        </li>
        </ul>

    
</div>
    </body>
</html>